{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6664ebc6-6bb1-4a00-9e41-85455f040051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scopes: ['https://www.googleapis.com/auth/cloud-platform']\n"
     ]
    }
   ],
   "source": [
    "#!pip install google-generativeai\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "API_KEY='AIzaSyBqgKjrXkuU5hU-PiYokFucduySJyUewR0'\n",
    "api_keypath_sa='badri-gcp-vertexai-6a4b740168ef.json'\n",
    "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    api_keypath_sa,\n",
    "    scopes=SCOPES\n",
    ")\n",
    "\n",
    "print(\"Scopes:\", credentials.scopes)\n",
    "\n",
    "PROJECT_ID = \"badri-gcp-vertexai\"\n",
    "REGION = 'us-central1'\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(\n",
    "    project = PROJECT_ID,\n",
    "    location= REGION,  # or the region where your Vertex AI is deployed\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890e4a08-9831-4773-9bc2-ebd375eb8c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Delhi is the capital of India.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure with API key\n",
    "genai.configure(api_key=\"AIzaSyC-u6YfpQP_uQsmaPDWEQIWgX3bnu8FgtE\")\n",
    "\n",
    "# âœ… Correct model name\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro\")\n",
    "\n",
    "# Send prompt\n",
    "#response = model.generate_content(\"Explain Vertex AI in simple terms.\")\n",
    "response = model.generate_content(\"Capital of india\")\n",
    "\n",
    "# Print the result\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b05b4b0-698f-4cb4-bdb1-c93402d30fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid input type. Expected a `dict` or `GenerationConfig` for `generation_config`.\nHowever, received an object of type: <class 'vertexai.generative_models._generative_models.GenerationConfig'>.\nObject Value: temperature: 0.7\ntop_p: 0.8\ntop_k: 40\nmax_output_tokens: 256\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     10\u001b[0m gen_config \u001b[38;5;241m=\u001b[39m GenerationConfig(\n\u001b[0;32m     11\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,         \u001b[38;5;66;03m# controls randomness\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,   \u001b[38;5;66;03m# limits response length\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,               \u001b[38;5;66;03m# nucleus sampling\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m                 \u001b[38;5;66;03m# limits to top-k tokens\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Generate content with config\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain Quantum Computing in simple terms.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgen_config\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\generative_models.py:305\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents must not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 305\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    306\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[0;32m    307\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    308\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[0;32m    309\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    310\u001b[0m     tool_config\u001b[38;5;241m=\u001b[39mtool_config,\n\u001b[0;32m    311\u001b[0m )\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mcontents \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mcontents[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrole:\n\u001b[0;32m    314\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontents[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m=\u001b[39m _USER_ROLE\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\generative_models.py:156\u001b[0m, in \u001b[0;36mGenerativeModel._prepare_request\u001b[1;34m(self, contents, generation_config, safety_settings, tools, tool_config)\u001b[0m\n\u001b[0;32m    152\u001b[0m     tool_config \u001b[38;5;241m=\u001b[39m content_types\u001b[38;5;241m.\u001b[39mto_tool_config(tool_config)\n\u001b[0;32m    154\u001b[0m contents \u001b[38;5;241m=\u001b[39m content_types\u001b[38;5;241m.\u001b[39mto_contents(contents)\n\u001b[1;32m--> 156\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m generation_types\u001b[38;5;241m.\u001b[39mto_generation_config_dict(generation_config)\n\u001b[0;32m    157\u001b[0m merged_gc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generation_config\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    158\u001b[0m merged_gc\u001b[38;5;241m.\u001b[39mupdate(generation_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\types\\generation_types.py:224\u001b[0m, in \u001b[0;36mto_generation_config_dict\u001b[1;34m(generation_config)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_config\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type. Expected a `dict` or `GenerationConfig` for `generation_config`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHowever, received an object of type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generation_config)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject Value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid input type. Expected a `dict` or `GenerationConfig` for `generation_config`.\nHowever, received an object of type: <class 'vertexai.generative_models._generative_models.GenerationConfig'>.\nObject Value: temperature: 0.7\ntop_p: 0.8\ntop_k: 40\nmax_output_tokens: 256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "genai.configure(api_key='AIzaSyC-u6YfpQP_uQsmaPDWEQIWgX3bnu8FgtE')\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")\n",
    "\n",
    "\n",
    "# Define generation config\n",
    "gen_config = GenerationConfig(\n",
    "    temperature=0.7,         # controls randomness\n",
    "    max_output_tokens=256,   # limits response length\n",
    "    top_p=0.8,               # nucleus sampling\n",
    "    top_k=40                 # limits to top-k tokens\n",
    ")\n",
    "\n",
    "# Generate content with config\n",
    "response = model.generate_content(\n",
    "    \"Explain Quantum Computing in simple terms.\",\n",
    "    generation_config=gen_config\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "#response = model.generate_content(\"Explain Vertex AI in simple terms.\",stream=True)\n",
    "#for r in response: \n",
    " #   print(r.text,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589f84d3-f6ab-4a8d-8166-3bf1e2aa4c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a regular computer bit like a light switch: it can be either ON (1) or OFF (0).  A quantum bit, or qubit, is like a special light switch that can be ON, OFF, or *both at the same time*.  This \"both at the same time\" state is called superposition.\n",
      "\n",
      "Think of it like flipping a coin: while it's spinning in the air, it's both heads and tails *potentially*, but only when it lands does it become definitely one or the other.  Qubits are similar â€“ they exist in a probabilistic state until measured.\n",
      "\n",
      "Another key concept is entanglement.  This is where two or more qubits become linked, even if they're far apart.  If you measure the state of one entangled qubit, you instantly know the state of the other, no matter the distance.  It's like having two coins that are magically linked; if one lands on heads, you know the other will be tails, even if they're in different rooms.\n",
      "\n",
      "Because qubits can be in multiple states at once, and because of entanglement, quantum computers can explore many possibilities simultaneously. This allows them to potentially solve certain types of problems much faster than regular computers, like:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key='AIzaSyC-u6YfpQP_uQsmaPDWEQIWgX3bnu8FgtE')  # Your API key\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"Explain Quantum Computing in simple terms.\",\n",
    "    generation_config={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 40,\n",
    "        \"max_output_tokens\": 256\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3964f5-9766-4745-9462-1833b65df40f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: parts {\n",
      "  text: \"Why don\\'t scientists trust atoms? \\n\\nBecause they make up everything!\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n",
      "Candidate 2: parts {\n",
      "  text: \"Why don\\'t scientists trust atoms? \\n\\nBecause they make up everything!\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n",
      "Candidate 3: parts {\n",
      "  text: \"Why don\\'t scientists trust atoms? \\n\\nBecause they make up everything!\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai.types import GenerationConfig  # âœ… Correct import\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key='AIzaSyC-u6YfpQP_uQsmaPDWEQIWgX3bnu8FgtE')\n",
    "\n",
    "# Load the Gemini model\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")\n",
    "\n",
    "# Correct GenerationConfig object\n",
    "config = GenerationConfig(\n",
    "    candidate_count=3,\n",
    "    stop_sequences=[\"\\nHuman:\"],\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=1000\n",
    ")\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content(\"Tell me a joke\", generation_config=config)\n",
    "\n",
    "# Display each candidate response\n",
    "for i, candidate in enumerate(response.candidates):\n",
    "    print(f\"Candidate {i+1}: {candidate.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e9bad-fa63-4572-aee9-569c5f799099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
